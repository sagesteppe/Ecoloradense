knitr::opts_chunk$set(
comment = "", echo = FALSE, message= FALSE, warning = FALSE
)
library(tidyverse)
library(ranger)
library(sf)
source('functions.R')
set.seed(27)
p <- file.path('..', 'rei_case_study_data')
store_perf <- st_read(file.path('..', 'data', 'store_performance.gpkg'), quiet = TRUE) |>
mutate(X2023_Revenue = X2023_Revenue/1e6, X2028_Revenue = X2028_Revenue/1e6)
knitr::opts_chunk$set(
comment = "", echo = FALSE, message= FALSE, warning = FALSE
)
library(tidyverse)
library(ranger)
library(sf)
source('functions.R')
set.seed(27)
p <- file.path('..', 'rei_case_study_data')
store_perf <- st_read(file.path('..', 'data', 'store_performance.gpkg'), quiet = TRUE) |>
mutate(X2023_Revenue = X2023_Revenue/1e6, X2028_Revenue = X2028_Revenue/1e6)
msa <- st_read(file.path('..', 'data', 'msa.gpkg'), quiet = TRUE) |>
# we are gonna strip down some of the names so info prints nicely to docs
mutate(name = str_trim(str_remove(name, 'Metro.*$|Micro.*$'))) |>
rename(tot_pop = basic_total_population)
shop_cntrs <- read.csv(file.path('..', 'rei_case_study_data', 'Shopping_Center_Locations.csv')) |>
st_as_sf(coords = c('longitude', 'latitude'), crs = 4326) |>
select(geo_id = geoid, zipcode_postalcode, cbsa_name, property_name)
nbrs <- st_read(file.path('..', 'data', 'neighbors_4prediction.gpkg'), quiet = TRUE) |>
filter(NB_Order == 'Third') |>
select(REI_StoreID)
zips <- st_read(file.path('..', 'data', 'zips.gpkg'))
competitors <- st_read(file.path('..', 'data', 'competitors.gpkg'), quiet = TRUE) |>
# actually on 72 hours this is gonna take forever, use our categories
select(-list_name)
colleges <- st_read(file.path('..', 'data', 'colleges.gpkg'), quiet = T)
rm(p)
store_perf <- st_join(
store_perf,
select(msa, MSA_name = name)
) |>
add_count(MSA_name)
store_perf |>
select(MSA_name, Pct_Change_Revenue, n) |>
st_drop_geometry() |>
arrange(desc(Pct_Change_Revenue), n)
# what i want is to calculate the percent growrth across all stores in an MSA
#  otherwise we just see New York, Seattle, Chicago and Atlanta,
# and we know that each of these areas has  astore which is not doing well.
store_perf_msa <- store_perf |>
group_by(MSA_name) |>
mutate(
n_stores_msa = n(),
MSA_pct_growth =
((sum(X2028_Revenue) - sum(X2023_Revenue)) / sum(X2023_Revenue)) * 100
) |>
arrange(desc(MSA_pct_growth)) |>
distinct(MSA_name, .keep_all = TRUE) |>
select(MSA_pct_growth, MSA_name, Pct_Change_Revenue, n_stores_msa)
stores_per_person <- left_join(
st_drop_geometry(select(store_perf, MSA_name)),
st_drop_geometry(select(msa, name, tot_pop)),
by = c('MSA_name' = 'name')
) |>
group_by(MSA_name) |>
add_count() |>
distinct(.keep_all = TRUE) |>
mutate(StoresPerPerson = tot_pop/n) |>
filter(n > 1) |>
arrange(desc(StoresPerPerson))
ggplot(data = stores_per_person, aes(x = StoresPerPerson, y = MSA_name)) +
geom_col() +
theme_minimal()
rm(stores_per_person)
# let's focus on stores with > 25% growth across them
msa_candidates <- store_perf_msa |>
filter(MSA_pct_growth > 25)  |>
left_join(
st_drop_geometry(select(msa, name, tot_pop)),
by = c('MSA_name' = 'name')
) |>
# and now filter down to metros with a cool and casual 1m people in excess of the
# number of REI stores they have...
filter(tot_pop > (n_stores_msa*1e6)+1e6 )
msa_candidates |>
sf::st_drop_geometry() |>
select(MSA = MSA_name, Growth = MSA_pct_growth, Stores = n_stores_msa, Pop. = tot_pop) |>
knitr::kable()
rm(store_perf_msa)
msa_candidates <- filter(msa_candidates, MSA_name != 'Los Angeles-Long Beach-Anaheim, CA')
shop_cntrs <- filter(shop_cntrs, cbsa_name %in% msa_candidates$MSA_name)
# OK first take away... THERE ARE ALOT OF THESE.
# now let's remove spots that are within any of the 3rd order neighboring zipcodes
# of our existing stores.
stores_in_msa <- filter(store_perf, MSA_name %in% msa_candidates$MSA_name)
# subset to the nbrs which are essential to our central stores, we will erase
# these from the metro areas, before calculating values for them.
nbrs <- filter(nbrs, REI_StoreID %in% unique(stores_in_msa$REI_StoreID))
# find the other potential zip codes in these areas.
msa <- filter(msa, name %in% msa_candidates$MSA_name)
msa <- st_difference(msa, st_union(nbrs))
zips <- zips[unlist(st_intersects(msa, zips)),]
# now subset our shopping cntrs again to possible target zip codes
shop_cntrs <- filter(shop_cntrs, zipcode_postalcode %in% zips$geo_id)
states <- tigris::states(progress_bar = FALSE) |>
st_transform(5070)
states <- states[unique(unlist(st_intersects(st_convex_hull(st_union(msa)), states))),]
ggplot() +
geom_sf(data = states) +
geom_sf(data = msa, fill = 'blue') +
geom_sf(data = nbrs, fill = 'red')  +
theme_minimal()
rm(states)
# well ran the 3 nbs model once and realize something... It was limited in it's
# train data to target areas. Hence the estimates of market segment it made
# were biased. So we need some other proxies.
zipsf <- filter(
zips,
basic_total_population > 40000 &
hhi_mild + hhi_med + hhi_upper > 15000 &
raceethnicity_cntpop_white + raceethnicity_cntpop_asian > 25000)
buf_dist <- units::set_units(units::set_units(10, 'miles'), 'km')
zipsf_l <- split(zipsf, f = 1:nrow(zipsf))
neighbrs <- lapply(zipsf_l, FUN = NBRs, zips, buf_dist)
neighbrs <- lapply(zipsf_l, FUN = NBRs, zips, buf_dist)
names(neighbrs) <- zipsf$geo_id
neighbrs <- bind_rows(neighbrs, .id = 'geo_id') %>%
mutate(
across(FT_ENROLL:Hunting_ct, ~ replace_na(.x, 0)),
age_median_age =
if_else(is.na(age_median_age), mean(age_median_age, na.rm = TRUE), age_median_age))|>
relocate(NB_Order, .after = geo_id)
View(neighbrs)
st_write(neighbrs, file.path('..', 'data', 'PossibleNeighbsExistingMSA.gpkg'))
rm(zipsf_l, buf_dist)
st_write(neighbrs, file.path('..', 'data', 'PossibleNeighbsExistingMSA.gpkg'), append = F)
rm(zipsf_l, buf_dist)
rm(colleges, competitors)
neighbrs <- st_read(file.path('..', 'data', 'PossibleNeighbsExistingMSA.gpkg'))
neighbrs <- filter(neighbrs, NB_Order == 'Third') |>
sf::st_drop_geometry()
rev2023_mod <- readRDS(file.path('..', 'models', 'nb3rd_2023_Revenue_rf.rda'))$model
rev2023_mod <- readRDS(file.path('..', 'models', 'nb3rd_2023_Revenue_rf.rda'))$model
rev2028_mod <- readRDS(file.path('..', 'models', 'nb3rd_2028_Revenue_rf.rda'))$model
rev2023_mod <- readRDS(file.path('..', 'models', 'nb3rd_2023_Revenue_rf.rda'))$model
rev2028_mod <- readRDS(file.path('..', 'models', 'nb3rd_2028_Revenue_rf.rda'))$model
preds2023 <- predict(rev2023_mod,  neighbrs, type = "se", se.method = "infjack" )
preds2028 <- predict(rev2028_mod,  neighbrs, type = "se", se.method = "infjack" )
msa_zip_options <- select(neighbrs, geo_id,
basic_total_population,
hhi_mild, hhi_med, hhi_upper,
age_cntpop_thirties, age_cntpop_middle, age_cntpop_career
) |>
mutate(
PredictedRev2023 = preds2023$predictions,
PredictedRev2023se = preds2023$se,
PredictedRev2028 = preds2028$predictions,
PredictedRev2028se = preds2028$se,
geo_id = as.numeric(geo_id)
)
zips_f <- zips[ unlist(st_intersects(msa, st_centroid(zips))),  'geo_id']
nf <- st_nearest_feature(zips_f, msa)
zips_f <- mutate(zips_f,
name = st_drop_geometry(msa)[nf,'name']
) |>
st_drop_geometry()
possibles <- left_join(msa_zip_options, zips_f, by = 'geo_id') |>
drop_na()
View(possibles)
rm(neighbrs_l, rev2023_mod, rev2028_mod, preds2023, preds2028, nf)
possibles |>
# search for growth in predicted revenues.
filter( (PredictedRev2023 + PredictedRev2023se) < (PredictedRev2028 - PredictedRev2028se)) |>
group_by(name) |>
slice_max(PredictedRev2028, n = 5) |>
select(geo_id, name, PredictedRev2028)
possibles
write.csv(possibles, file.path('..', 'results', 'CandidatesExistingMsa.csv'), row.names = FALSE )
#' @param se_prediction boolean, whether to predict the SE surfaces or not, can add roughly
#' a week onto the prediction at 3m.
#' @param p2proc path to the processed raster data.
#' @param train_split Numeric. The proportion of data to use for train, at first iteration
#' a standard 0.8, good, but lot's of data are available for test later so up to 0.9 good.
#' @param PAratio Numeric. The ratio of presence to absences - simply for appending to file name,
#' the exact quantity are saved in model objects. "1:2"
#' @param distOrder Character. The distance between the nearest absence to presence, as a multiple of the cell resolution
#' e.g. distOrder 1 at a 90m resolution indicates the nearest absence is >90m away from a presence, at 10m it indicates > 10m away.
#' @param remove_tiles Boolean. Defaults to FALSE, whether to remove tiles required for model prediction (applies only to high resolution data sets).
modeller <- function(x, resolution, iteration, se_prediction, p2proc, train_split,
PAratio, distOrder, remove_tiles){
if(missing(remove_tiles)){remove_tiles <- FALSE}
if(missing(se_prediction)){se_prediction <- FALSE}
rast_dat <- rastReader(paste0('dem_', resolution), p2proc)
cores <- parallel::detectCores()
df <- dplyr::bind_cols(
dplyr::select(x, Occurrence),
dplyr::select(terra::extract(rast_dat, x), -ID),
) |>
tidyr::drop_na() %>%
dplyr::mutate(
Occurrence = as.factor(Occurrence)) |>
sf::st_drop_geometry()
fname <- paste0(resolution, '-Iteration', iteration, '-PA', PAratio, distOrder)
#######################         MODELLING           ##########################
# this is the fastest portion of this process. It will take at most a few minutes
# at any resolution, the goal of this paper wasn't really focused on comparing
# multiple models of families nor messing with parameters, so we use Random Forest
# simply because they work very well with default settings, almost 'controlling'
# for stochasticity in this portion of the work.
# only rerun modelling if a saved .rds object doesn't exist.
if(file.exists(paste0('../results/models/', fname, '.rds'))){
rf_model <- readRDS(paste0('../results/models/', fname, '.rds'))
message(
'An exising model for this resolution and iteration already exists;
reloading it now for prediction')
} else {
# split the input data into both an explicit train and test data set.
TrainIndex <- caret::createDataPartition(
df$Occurrence, p = train_split, list = FALSE, times = 1)
Train <- df[TrainIndex,]; Test <- df[-TrainIndex,]
Train.sf <- sf::st_as_sf(Train, coords = c('Longitude', 'Latitude'), crs = 32613)
## remove totally uninformative features using Boruta analysis, these features
# can only hinder the model, and make prediction onto raster surfaces take longer
BorutaRes <- Boruta::Boruta(Occurrence ~ ., data = Train, num.threads = cores, doTrace = 1)
importance <- Boruta::attStats(BorutaRes)
rn <- rownames(importance)
important_vars <- Boruta::getSelectedAttributes(BorutaRes, withTentative = TRUE)
keep <- unique(c('Occurrence', important_vars))
Train <- Train[ , names(Train) %in% keep]
# develop a cross validation structure which is explicitly spatial
indices_knndm <- CAST::knndm(Train.sf, rast_dat, k=10)
# Now we will tune the hyperparameters for this model.
tgrid <- expand.grid(
mtry =
floor(ncol(Train)/ 2.4):floor(ncol(Train)/ 1.6),
splitrule = 'gini',
min.node.size = c(1:9, seq(10, 30, by = 5))
)
# calculate class weights for the factor levels.
wts = c(
1 - sum(Train$Occurrence==0)/nrow(Train),
1 - sum(Train$Occurrence==1)/nrow(Train)
)
message('Tuning hyperparameters.')
model <- caret::train(
x = Train[,-grep('Occurrence', colnames(Train))],
y = Train$Occurrence,
method = "ranger",
num.trees = 750,
metric = 'Accuracy',
keep.inbag = TRUE,
class.weights = wts,
importance = 'permutation',
trControl = caret::trainControl(
method="cv",
index = indices_knndm$indx_train,
savePredictions = "final",
allowParallel = TRUE)
)
rf_model <- ranger::ranger(
Occurrence ~ ., data = Train, probability = T, keep.inbag = TRUE,
mtry = model[['finalModel']][['mtry']],
min.node.size = model[['finalModel']][['min.node.size']],
importance = 'permutation',
class.weights = wts
)
# save the model
saveRDS(rf_model,
file = paste0('../results/models/', fname, '.rds'))
# save the test data.
write.csv(Test, paste0('../results/test_data/', fname, '.csv'))
# threshold the model and use these values for binary classification estimates
predictions <- predict(rf_model, Test, type = 'se', se.method = 'infjack', probability=TRUE)
e <- dismo::evaluate(
p = predictions$predictions[Test$Occurrence==1,2],
a = predictions$predictions[Test$Occurrence==0,2]
)
th <- dismo::threshold(e)
saveRDS(th, file = paste0('../results/evaluations/', fname, '-thresh.rds'))
saveRDS(e, file = paste0('../results/evaluations/', fname, '-eval.rds'))
# save the confusion matrix
predictions <- predict(rf_model, Test, type = 'se', se.method = 'infjack', probability=TRUE)
predictions$binary <- as.factor(if_else(predictions$predictions[,2] <= th$spec_sens, 0, 1))
cmRestrat <- caret::confusionMatrix(predictions$binary, Test$Occurrence,
positive = '1', mode = 'everything')
saveRDS(cmRestrat,
file = paste0('../results/tables/', fname, '.rds'))
# we will also save the pr-auc and ROC-auc metrics.
df_auc <- data.frame(
truth = as.factor(Test$Occurrence),
Class1 = predictions$predictions[,2]
)
pr_auc_val <- yardstick::pr_auc(
data = df_auc, truth = truth, Class1, event_level = 'second')
roc_auc_val <- yardstick::roc_auc(
df_auc, truth, Class1, event_level = 'second')
setNames(
data.frame(
rbind(
pr_auc_val,
roc_auc_val
)
), nm = c('metric', 'estimator', 'estimate')
) |>
mutate(resolution = resolution, iteration = iteration) |>
write.csv(paste0('../results/tables/', fname, '.csv'),
row.names = F)
rm(df, df_auc, TrainIndex, Train, Test, predictions, cmRestrat)
}
pout <- '../results/suitability_maps'
###################      PREDICT ONTO SURFACE        #########################
# the prediction is generally straightforward, it doesn't take an obscene
# amount of RAM and can happen relatively quickly; overnight for the
# higher resolution data products.
# there is some weird corner case where we could not skip to the post prediction steps:
# AOA, and SE.
if(!file.exists(file.path(pout, paste0(fname, '-Pr.tif')))){
pr <- function(...) predict(..., type = 'response', num.threads = 1)$predictions
if(resolution %in% c('3arc', '1arc')){ntile = 1} else
if(resolution == '1-3arc'){ntile = 2} else {ntile = 4}
if(ntile > 1){
tile_path_4pr <- file.path(pout, paste0('tilesPR', '_', resolution))
if(file.exists(tile_path_4pr)){
message('Tiles already created for this resolution, skipping to prediction!\n')
} else {
dir.create(tile_path_4pr, showWarnings = F); message('Making tiles for prediction')
template <- rast(nrows = ntile, ncols = ntile, extent = ext(rast_dat), crs = crs(rast_dat))
terra::makeTiles(rast_dat, template, filename = file.path(tile_path_4pr, "tile_.tif"))
rm(template)
}
tiles <- file.path(tile_path_4pr, list.files(tile_path_4pr))
pr_tile_path <- file.path(pout, paste0('pr_tiles', '_', resolution, iteration))
dir.create(pr_tile_path, showWarnings = F)
message('Writing Predicted Probability to Raster using tiles - this while take some time')
pb <- txtProgressBar(
min = 0,  max = length(tiles), style = 3, width = 50, char = "+")
for (i in seq_along(tiles)){
if(!file.exists(file.path(pr_tile_path, paste0(i, '.tif')))){
terra::predict(
rast(tiles[i]), rf_model, f = pr,
filename = file.path(pr_tile_path, paste0(i, '.tif')),
overwrite = T, na.rm=TRUE)
setTxtProgressBar(pb, i)
gc(verbose = FALSE)
}
close(pb)
}
mos <- terra::mosaic(sprc(
file.path(pr_tile_path, list.files(pr_tile_path))
), fun = 'mean')
writeRaster(
mos[[2]],
wopt = c(names = 'Probability'),
filename = file.path(pout, paste0(fname, '-Pr.tif')))
rm(mos)
# this should be an OPTION, which defaults to FALSE.
if(remove_tiles == TRUE){
unlink(file.path(pout, 'pr_tiles')) # can remove the tiles we used for the probability surface.
}
} else { # in these cases, we only need to use a single tile for prediction, we can
# just use the existing virtual raster to do this.
if(!file.exists(file.path(pout, paste0(fname, '-Pr.tif')))){
message('Writing Predicted Probability to Raster')
terra::predict( # rasters, skip straight to modelling.
cores = 1, f = pr,
rast_dat, rf_model, cpkgs = "ranger",
filename = file.path(pout, paste0(resolution, '-IterationCoarse', iteration, '.tif')),
wopt = c(names = 'predicted_suitability'), na.rm=TRUE,
overwrite = T)
writeRaster( # we wrote a raster with both predicted class probabilities onto it.
# we don't want both, we are going to 'rewrite' the raster so only one class remains.
rast(
file.path(
pout, paste0(resolution, '-IterationCoarse', iteration, '.tif')))[[2]],
file.path(pout, paste0(fname, '-Pr.tif')),
overwrite = T
)
file.remove(
file.path(
pout, paste0(resolution, '-IterationCoarse', iteration, '.tif')
)
)
} else {
'Predicted Probability Raster already exists - skipping.'}
}
unlink(file.path(pout, 'pr_tiles'))
gc(verbose = FALSE)
}
################ AREA OF APPLICABILITY SURFACE    ############################
message('Writing Area of Applicability to Raster')
if(!exists(rf_model)){
rf_model <- rf_model <- readRDS(paste0('../results/models/', fname, '.rds'))
}
AOAfun <- function(rf_model, r) {
CAST::aoa(r, model, LPD = FALSE, verbose=FALSE)$AOA
}
terra::predict( # rasters, skip straight to modelling.
cores = 1, f = AOAfun,
rast_dat, rf_model, cpkgs = "CAST",
filename = file.path(pout, paste0(fname, '-AOA', '.tif')),
wopt = c(names = 'AOA'), na.rm=TRUE,
overwrite = T)
###############      STANDARD ERROR PREDICTION        ########################
# the SE predictions are absurdly memory hungry, We will create 'tiles' to
# predict onto, and then combine them once the predictions are complete
# Note I use the term SE here, because it's what the R package uses, however,
# upon further reading what they are actually calculating is a Confidence
# interval - so great naming... https://github.com/imbs-hl/ranger/issues/136
if(se_prediction == TRUE){
tile_path_4se <- file.path(pout, paste0(resolution, 'TilesSE'))
final_se_path <- file.path(pout, paste0(resolution, 'SETiles'))
# it seems the tiles pretty much need to be under 200MB or so for safe CI prediction???
if(resolution == '3arc'){ntile = 2} else # 4 tiles
if(resolution == '1arc'){ntile = 6} else # 36 tiles Needed # earlier iteration with 16 FAILED # iter w/ 25 failed on tile 18...
if(resolution == '1-3arc'){ntile = 16}  # 144 /121/81 failed - follow under 200mb rule.
if(resolution == '3m'){
'Confidence Intervals cannot be produced for data of these size;
it would require over 2k tiles and 1 week of compute'}  else {
# don't create the tiles if they already exist.
if(exists(tile_path_4se)){
message('Tiles for SE exist at this resolution, skipping to predict.')} else {
message('Making tiles for prediction of standard errors')
dir.create(tile_path_4se, showWarnings = F)
dir.create(final_se_path, showWarnings = F)
template <- rast(nrows = ntile, ncols = ntile, extent = ext(rast_dat), crs = crs(rast_dat))
terra::makeTiles(rast_dat, template, filename = file.path(tile_path_4se, "tile_.tif"))
rm(template)
}
tiles <- file.path(tile_path_4se, list.files(tile_path_4se))
se <- function(...) predict(..., type = 'se', se.method = 'infjack',
predict.all = FALSE,
probability = TRUE, num.threads = cores/4)$se
# # predict the standard error onto the tiles.
# create and initialize progress bar
message('Predicting SE to tiles; this may take a really long time')
pb <- txtProgressBar(
min = 0,  max = length(tiles), style = 3, width = 50, char = "+")
for (i in seq_along(tiles)){
if(!file.exists(file.path(final_se_path, paste0('SE', i, '.tif')))){ # in case of crash, pick up where left off.
terra::predict(
rast(tiles[i]), rf_model, f = se,
filename = file.path(final_se_path, paste0('SE', i, '.tif')),
overwrite = T, na.rm=TRUE)
gc(verbose = FALSE)}
setTxtProgressBar(pb, i)
}
close(pb)
mos <- terra::mosaic(sprc(
file.path(final_se_path, list.files(final_se_path))
), fun = 'mean')
writeRaster(
mos[[2]],
wopt = c(names = 'standardError'),
filename = file.path(fname, '-SE.tif'),
overwrite = T)
gc(verbose = FALSE)
}
unlink(final_se_path)
}
}
library(sf)
library(tidyverse)
library(terra)
library(caret)
library(bonsai)
library(parsnip)
library(ranger)
library(lightgbm)
library(future)
library(dials)
library(finetune)
source('functions.R')
set.seed(23)
p <- '../data/Data4modelling'
f <- file.path(p, list.files(p))
abs <- st_read('../data/Data4modelling/iter1-pa.gpkg')
m30 <- sf::st_read("../data/Data4modelling/10m-presence-iter1.gpkg") %>%
rename(Occurrence = Presenc) |>
sf::st_as_sf()
### we know that 1:1 absence to presence is far too low when including the 'local'
# absences.
m30 <- bind_rows(m30, abs)
m30 <- filter(m30, st_is(m30, "POINT")) |>
select(Occurrence)
distOrder_PAratio_simulator(
x = m30, distOrder = 0, PAratio = 3.0,
resolution = 10, iteration = 1, se_prediction = FALSE,
train_split = 0.9, p2proc = '../data/spatial/processed'
)
distOrder_PAratio_simulator(
x = m30, distOrder = 0, PAratio = 3.6,
resolution = 10, iteration = 1, se_prediction = FALSE,
train_split = 0.9, p2proc = '../data/spatial/processed'
)
distOrder_PAratio_simulator(
x = m30, distOrder = 0, PAratio = 3.6,
resolution = 10, iteration = 1, se_prediction = FALSE,
train_split = 0.9, p2proc = '../data/spatial/processed'
)
distOrder_PAratio_simulator(
x = m30, distOrder = 0, PAratio = 3.6,
resolution = 10, iteration = 1, se_prediction = FALSE,
train_split = 0.9, p2proc = '../data/spatial/processed'
)
m30 <- filter(m30, st_is(m30, "POINT")) |>
select(Occurrence)
distOrder_PAratio_simulator(
x = m30, distOrder = 0, PAratio = 3.0,
resolution = 10, iteration = 1, se_prediction = FALSE,
train_split = 0.9, p2proc = '../data/spatial/processed'
)
