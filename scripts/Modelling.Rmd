---
title: "Modelling Eriogonum coloradense Presence and Abundance"
author: "steppe"
output: html_document
---

```{r}
library(sf)
library(tidyverse)
library(terra)
library(caret)
library(bonsai)
library(parsnip)
library(lightgbm)
library(future)
library(dials)
library(finetune)
source('functions.R')
set.seed(23)
```

```{r}
p <- '../data/Data4modelling'
f <- file.path(p, list.files(p))
```

Model presence for iteration 0, that is historic records only. 

Model presence for iteration 1, both the historic and 2024 ground truth data. 
```{r, eval = F}

abs <- st_read('../data/Data4modelling/iter1-pa.gpkg')
m30 <- sf::st_read("../data/Data4modelling/10m-presence-iter1.gpkg") %>% 
  rename(Occurrence = Presenc) |>
  sf::st_as_sf() 
### we know that 1:1 absence to presence is far too low when including the 'local'
# absences. 
m30 <- bind_rows(m30, abs)
m30 <- filter(m30, st_is(m30, "POINT")) |>
  select(Occurrence)


distOrder_PAratio_simulator(
  x = m30, distOrder = 0, PAratio = 3.9,
  resolution = 90, iteration = 1, se_prediction = FALSE,
  train_split = 0.9, p2proc = '../data/spatial/processed'
  )


```


# playing with count prediction 

```{r Partition data and clean for modelling}

# note that we have already multiplied these values out so that they went from a 
# 3x3m quadrat to the number of

ct <- st_read('../data/Data4modelling/30m-count-iter1.gpkg', quiet = TRUE) |>
  select(Prsnc_M, Prsnc_J, Lctn_bb)

# extract independent variables to data set . 
resolution = "1arc"
train_split = 0.8; p2proc = '../data/spatial/processed'
rast_dat <- rastReader(paste0('dem_', resolution), p2proc) 

r <- rast('../results/suitability_maps/1arc-Iteration1-PA1:3DO:2-Pr.tif')
names(rast_dat)

rast_dat

df <- dplyr::bind_cols(
  ct, 
  dplyr::select(terra::extract(rast_dat, ct), -ID), 
  Pr.SuitHab = terra::extract(r, ct)$X1
) |> 
  mutate(Prsnc_All = Prsnc_J + Prsnc_M, .before = 1) |>
  tidyr::drop_na() |>
  select(-Prsnc_J, -Prsnc_M)


data_split <- df |>
  rsample::initial_split(strata = Prsnc_All, prop = 4/5) 

train <- rsample::training(data_split)
test  <- rsample::testing(data_split)

# NOT CURRENTLY WORKING 
# indices_knndm <- CAST::knndm(
#   sf::st_as_sf(train, coords = c('Longitude', 'Latitude'), crs = 32613) |> select(-Prsnc_M),
#  rast_dat,
#  space = 'geographic',
#  k=10)

rm(ct)
```

Modelling count data has not been very successful, nor did the field work really develop any strong insights on what drives the abundance of the plants. 
Rather the abundance seemed random within populations, and between - with the exception of Cocheotopa where plants were much smaller and incredibly more abundant than elsewhere. 

The null hypothesis for estimating population sizes will be the use of the arithmetic mean per populations. 

```{r arithmetic mean of counts per population}

# this isn't a perfect comparision, because the locations more reflect how I hiked
# rather than a population. i.e. all visits in a day, unless i went back to truck
# between them were lumped into a single 'location'. so we can't report these values
# in the end. 

preds <- train %>% 
  group_by(Lctn_bb) %>% 
  summarize(Predicted = mean(Prsnc_All)) %>% 
  sf::st_drop_geometry()

arith_mean <- test %>% 
  select(Observed = Prsnc_All, Lctn_bb) |>
  sf::st_drop_geometry()

mean_preds <- left_join(arith_mean, preds, by = 'Lctn_bb')
Metrics::mae(mean_preds$Observed, mean_preds$Predicted)

ggplot(data = mean_preds) + 
  geom_point(aes(x = Observed, y = Predicted)) +
  theme(aspect.ratio = 1) +
  coord_fixed() + 
  geom_abline()

```

The use of kriging interpolation, where we simple smooth averages out between the plots, will be our null hypothesis for any spatial estimates of population size. 

```{r idw data}

krig_preds <- data.frame(
    'Observed' = test$Prsnc_All, 
    'Predicted' = gstat::krige(Prsnc_All ~ 1, train, newdata = test)$var1.pred,
    'Pr.suit' = test$Pr.SuitHab
)

ggplot(data = krig_preds) + 
  geom_point(aes(x = Observed, y = Predicted, color = Pr.suit)) +
  theme(aspect.ratio = 1) +
  coord_fixed() + 
  geom_abline()

Metrics::mae(krig_preds$Observed, krig_preds$Predicted)
```


```{r Split data for modelling}
train <- sf::st_drop_geometry(train) |>
  select(-Lctn_bb)
test  <- rsample::testing(data_split)

rec <- recipes::recipe(Prsnc_All ~ ., data = train) 
rs <- rsample::bootstraps(train, times = 10)

tune_gr <- parsnip::boost_tree( 
  mode = 'regression',
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(), 
  loss_reduction = tune(),
  learn_rate = tune(),
  stop_iter =tune()
)

```

While random forests are relatively robust against the presence of uninformative features, gradient boosting methods are less so.
Here we will use recursive feature elimination to drop some variables from our data set. 

```{r Feature Selection}

ctrl <- caret::rfeControl(
  functions = caret::treebagFuncs, method = "repeatedcv", repeats = 10,
  allowParallel = TRUE)

future::plan(multisession, workers = parallel::detectCores())
rfProfile <- caret::rfe(
  train[,2:ncol(train)], train$Prsnc_All,
  rfeControl = ctrl, metric = 'MAE'
  )

var <- rfProfile$variables
var <- var[var$Variables==8,] |>
  group_by(var) |>
  summarise(Overall_mean = mean(Overall)) |>
  arrange(-Overall_mean) |>
  slice_max(n = 8, order_by = Overall_mean) |>
  pull(var)

train <- select(train, all_of(c('Prsnc_All', var)))
```

As we can see from the table above, fewer variables work considerably better. 
We reduce the mean absolute error by nearly half a plant per quadrant! 

```{r xgboost poisson}

xgb_poisson <- tune_gr  |>
  parsnip::set_engine("xgboost", objective = "count:poisson") 

my_params <- parameters( # it's a pretty basic grid for running xgboost 
  list( # with small sample sizes, essentially we try to get at preventing
    trees(range = c(1L, 1200L)),  # overfitting in two ways. - by reducing the
    tree_depth(range = c(2L , 8L)), # tree depth, and the number of trees grown
    min_n(range = c(2L, 20)),
    learn_rate(range = c(0.01, 0.3)),
    loss_reduction(range = c(0, 20)),
    stop_iter(range = c(3, 20))
  ) # although it's pretty simple I am quite confident we have reduced the MAE
) # of our models by 20-25% on our independent test data using these restrictions.

xgb_gr <- my_params %>% 
  grid_regular(levels = 3)

future::plan(multisession, workers = parallel::detectCores())
params_xg_poisson <- xgb_poisson |>
      tune::tune_grid(
        rec,
        resamples = rs,
        metrics = yardstick::metric_set(yardstick::mae), 
        grid = xgb_gr
      )

best_xg_pois <- tune::select_best(params_xg_poisson, metric = "mae")

xg_poisson <- xgb_poisson |>
  tune::finalize_model(best_xg_pois)

xgb_pois_fit <- xg_poisson %>% 
  fit(Prsnc_All ~ ., data = train)

xbg_pois_preds <- data.frame(
    'Observed' = test$Prsnc_All, 
    'Predicted' = stats::predict(xgb_pois_fit, new_data = test),
    'Pr.suit' = test$Pr.SuitHab
)

ggplot(data = xbg_pois_preds) + 
  geom_point(aes(x = Observed, y = .pred, color = Pr.suit)) +
  theme(aspect.ratio = 1) 

vip::vip(xgb_pois_fit)
Metrics::mae(xbg_pois_preds$Observed, xbg_pois_preds$.pred)

rm(xgb_poisson, xgb_poisson_gr)
```

There are a good number of zeros, but not enough to warrant a zero inflated negative binomial. 
We will try a negative binomial objective. 
```{r xgboost negative binomial}
1+1
```

Tweedie needs to be allowed to explore the hyperparameters more than poisson. 

```{r xgboost with tweedie distribution, eval = F}

xgb_tweedie_model <- tune_gr |>
    parsnip::set_engine("xgboost", objective = "reg:tweedie")

xgb_poisson_gr <- xgb_tweedie_model |>
  # going to use a typical grid which can support subgrid searchers. 
  extract_parameter_set_dials() |>
  dials::grid_regular(levels = 3)
  
future::plan(multisession, workers = parallel::detectCores())
xbg_tweedie_params <- xgb_tweedie_model |>
      finetune::tune_race_anova(
        rec,
        resamples = rs,
        metrics = yardstick::metric_set(yardstick::mae), 
        grid = xgb_poisson_gr
      )

best_param_tweedie <- tune::select_best(xbg_tweedie_params, metric = "mae")

xgb_tweedie_mod <- xgb_tweedie_model |>
  tune::finalize_model(best_param_tweedie)

xgb_tweedie_fit <- xgb_tweedie_mod %>% 
  fit(Prsnc_All ~ ., data = train)

xbg_tweedie_preds <- data.frame(
    'Observed' = test$Prsnc_All, 
    'Predicted' = stats::predict(xgb_tweedie_fit, new_data = test),
    'Pr.suit' = test$Pr.SuitHab
)

ggplot(data = xbg_tweedie_preds) + 
  geom_point(aes(x = Observed, y = .pred, color = Pr.suit)) +
  theme(aspect.ratio = 1) 

vip::vip(xgb_tweedie_fit)
Metrics::mae(xbg_tweedie_preds$Observed, xbg_tweedie_preds$.pred)
```

Make a table of results 
```{r}

data.frame(
  Model = c('Arithmetic Mean', 'Kriging', 'XGB Poisson', 'XGB Tweedie'), 
  MAE = c(
    Metrics::mae(mean_preds$Observed, mean_preds$Predicted),
    
    
    )
)


Metrics::rmse(xbg_pois_preds$Observed, xbg_pois_preds$.pred)
Metrics::rmse(xbg_tweedie_preds$Observed, xbg_tweedie_preds$.pred)

Metrics::mae(mean_preds$Observed, mean_preds$Predicted)
Metrics::mae(krig_preds$Observed, krig_preds$Predicted)
Metrics::mae(xbg_pois_preds$Observed, xbg_pois_preds$.pred)
Metrics::mae(xbg_tweedie_preds$Observed, xbg_tweedie_preds$.pred)
```





```{r lightgbm}

tune_gr <- parsnip::boost_tree( 
  mode = 'regression'
)

lgbm_poisson_gr <- tune_gr  |>
  parsnip::set_engine("lightgbm", objective = "reg:squarederror")

bt_grid <- lgbm_poisson_gr %>%
  # going to use a typical grid which can support subgrid searchers. 
  extract_parameter_set_dials() %>% 
  dials::grid_regular(levels = 2)

future::plan(multisession, workers = parallel::detectCores())
lgb_poisson_params <- lgb_poisson_model |>
      finetune::tune_race_anova(
        rec,
        resamples = rs,
       metrics = yardstick::metric_set(yardstick::mae), 
        grid = bt_grid
      )
best_param <- tune::select_best(model_params, metric = "mae")

```

Try with mboost

```{r try with gbm3}

trainControl <- trainControl(method="repeatedcv", number=10, repeats = 5)

  gbm_model <- caret::train(
    x = train[,-grep('Prsnc_All', colnames(train))],
    y = train$Prsnc_All,
    method = "gbm",
    distribution="poisson",
    metric = 'MAE', 
    trControl = caret::trainControl(
      method="repeatedcv", number=10, repeats = 5,
      allowParallel = TRUE)
    )
  
  
  gbm_pred <- data.frame(
    'Observed' = test$Prsnc_All, 
    'Predicted' = predict(gbm_model, newdata=test, type="raw"), 
    'Pr.suit' = test$Pr.SuitHab
)
  
  
Metrics::mae(gbm_pred$Observed, gbm_pred$Predicted)
Metrics::rmse(gbm_pred$Observed, gbm_pred$Predicted)

ggplot(data = gbm_pred) + 
  geom_point(aes(x = Observed, y = Predicted, color = Pr.suit)) +
  theme(aspect.ratio = 1) +
  coord_fixed() + 
  geom_abline()

```



