---
title: "Modelling Eriogonum coloradense Presence and Abundance"
author: "steppe"
output: html_document
---

```{r}
library(sf)
library(tidyverse)
library(terra)
library(caret)
source('functions.R')
set.seed(23)
```

```{r}
p <- '../data/Data4modelling'
f <- file.path(p, list.files(p))
```

Model presence for iteration 0, that is historic records only. 

Model presence for iteration 1, both the historic and 2024 ground truthed data. 
```{r, eval = F}

abs <- st_read('../data/Data4modelling/iter1-pa.gpkg')
m30 <- sf::st_read("../data/Data4modelling/30m-presence-iter1.gpkg") %>% 
  rename(Occurrence = Presenc) |>
  sf::st_as_sf() 
### we know that 1:1 absence to presence is far too low when including the 'local'
# absences. 
m30 <- bind_rows(m30, abs)
m30 <- filter(m30, st_is(m30, "POINT")) |>
  select(Occurrence)

distOrder_PAratio_simulator(
  x = m30, distOrder = 2, PAratio = 3.0,
  resolution = 30, iteration = 1, se_prediction = FALSE,
  train_split = 0.9, p2proc = '../data/spatial/processed'
  )


process_plss(
  path = '.', 
  pathOut = '../geodata', 
  tile_cells = tile_cells 
)

getwd()

```



# playing with count prediction 


```{r}

ct <- st_read('../data/Data4modelling/30m-count-iter1.gpkg') |>
  select(Prsnc_M)

resolution = "1arc"
train_split = 0.8; p2proc = '../data/spatial/processed'
  
rast_dat <- rastReader(paste0('dem_', resolution), p2proc) 
cores <- parallel::detectCores()
  
  df <- dplyr::bind_cols(
    ct, 
    dplyr::select(terra::extract(rast_dat, ct), -ID), 
  ) |> 
    tidyr::drop_na() 
  
  saveRDS(df, '../data/test_ct_data.Rds')
  df <- readRDS('../data/test_ct_data.Rds')
  
```



```{r xgboost}
r <- terra::rast('../results/suitability_maps/1arc-Iteration1-PA1:2.7DO:2-Pr.tif')
names(r) <- 'Pr.SuitHab'

df <- readRDS('../data/test_ct_data.Rds') 
df <- df |>
  sf::st_drop_geometry() |>
  bind_cols(
    Pr.SuitHab = terra::extract(r, df)$Pr.SuitHab
  )

# let's try and bring count back to it's 3x3m quadrat size, I think too big 
# of numbers throws xgboost off. 
df <- mutate(df, Prsnc_M = round(Prsnc_M / 100,0))

data_split <- df |>
  rsample::initial_split(strata = Prsnc_M, prop = 4/5)

train <- rsample::training(data_split)
test  <- rsample::testing(data_split)

indices_knndm <- CAST::knndm(
  sf::st_as_sf(train, coords = c('Longitude', 'Latitude'), crs = 32613) |> select(-Prsnc_M),
  rast_dat,
  space = 'geographic',
  k=10)


rec <- recipes::recipe(Prsnc_M ~ ., data = train) 
rs <- rsample::bootstraps(train, times = 15) 

xgb_reg_model <- parsnip::boost_tree( 
  mode = 'regression',
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(), 
  loss_reduction = tune(),
  learn_rate = tune(), 
  stop_iter = tune()
) |>
    parsnip::set_engine("xgboost", objective = "count:poisson")


doParallel::registerDoParallel() 
model_params <- xgb_reg_model |>
   finetune::tune_sim_anneal(rec, # about 10-15 minutes to run
                             # honestly rarely beat the initial starts. 
                             # even when greatly increasing the iterations. 
                     #        metrics = yardstick::metric_set(yardstick::mae),
                   resamples = rs, iter = 15, initial = parallel::detectCores()
                   )

best_param <- tune::select_best(model_params, metric = "rmse")

xgb_reg_model <- xgb_reg_model |>
  tune::finalize_model(best_param)

xgb_fit <- xgb_reg_model %>% 
  fit(Prsnc_M ~ ., data = train)

wflow <- workflows::workflow() %>% 
  workflows::add_model(xgb_reg_model) %>% 
  workflows::add_recipe(rec) 

test_pred <- data.frame(
    'Observed' = test$Prsnc_M, 
    'Predicted' = stats::predict(xgb_fit, new_data = test),
    'Pr.suit' = test$Pr.SuitHab
)


Metrics::mae(test_pred$Observed, test_pred$.pred)
Metrics::rmse(test_pred$Observed, test_pred$.pred)

ggplot(data = test_pred) + 
  geom_point(aes(x = Observed, y = .pred, color = Pr.suit)) +
  theme(aspect.ratio = 1) 

vip::vip(xgb_fit, num_features = 20)
```


```{r try with gbm3}

trainControl <- trainControl(method="repeatedcv", number=10, repeats = 5)

  gbm_model <- caret::train(
    x = train[,-grep('Prsnc_M', colnames(train))],
    y = train$Prsnc_M,
    method = "gbm",
    distribution="poisson",
    metric = 'MAE', 
    trControl = caret::trainControl(
      method="cv",
     # index = indices_knndm$indx_train,
      allowParallel = TRUE)
    )
  
  
  test_pred <- data.frame(
    'Observed' = test$Prsnc_M, 
    'Predicted' = predict(gbm_model, newdata=test, type="raw"), 
    'Pr.suit' = test$Pr.SuitHab
)
  
Metrics::mae(test_pred$Observed, test_pred$Predicted)
Metrics::rmse(test_pred$Observed, test_pred$Predicted)

ggplot(data = test_pred) + 
  geom_point(aes(x = Observed, y = Predicted, color = Pr.suit)) +
  theme(aspect.ratio = 1) +
  coord_fixed() + 
  geom_abline()

```


Try with mboost
```{r}

trainControl <- trainControl(method="repeatedcv", number=10, repeats = 5)

train[,-grep('Prsnc_M', colnames(train))] <- apply(
  train[,-grep('Prsnc_M', colnames(train))], 
  MARGIN = 2, 
  FUN = scales::rescale, to=c(0,1)
  )

  model_glmboost <- caret::train(
    x = train[,-grep('Prsnc_M', colnames(train))],
    y = train$Prsnc_M,
    method = "glmboost",
    family=mboost::Poisson(),
    metric = 'MAE', 
    trControl = caret::trainControl(
      method="cv",
 #     index = indices_knndm$indx_train,
      allowParallel = TRUE)
    )
  
summary(model)


  test_pred <- data.frame(
    'Observed' = test$Prsnc_M, 
    'Predicted' = predict(model_glmboost, newdata=test, type="raw"), 
    'Pr.suit' = test$Pr.SuitHab
)
  
Metrics::mae(test_pred$Observed, test_pred$Predicted)
Metrics::rmse(test_pred$Observed, test_pred$Predicted)

ggplot(data = test_pred) + 
  geom_point(aes(x = Observed, y = Predicted, color = Pr.suit)) +
  theme(aspect.ratio = 1) +
  coord_fixed() + 
  geom_abline()
```

