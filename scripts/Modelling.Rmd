---
title: "Modelling Eriogonum coloradense Presence and Abundance"
author: "steppe"
output: html_document
---

```{r}
library(sf)
library(tidyverse)
library(terra)
library(caret)
library(bonsai)
library(parsnip)
library(lightgbm)
library(future)
source('functions.R')
set.seed(23)
```

```{r}
p <- '../data/Data4modelling'
f <- file.path(p, list.files(p))
```

Model presence for iteration 0, that is historic records only. 

Model presence for iteration 1, both the historic and 2024 ground truth data. 
```{r, eval = F}

abs <- st_read('../data/Data4modelling/iter1-pa.gpkg')
m30 <- sf::st_read("../data/Data4modelling/10m-presence-iter1.gpkg") %>% 
  rename(Occurrence = Presenc) |>
  sf::st_as_sf() 
### we know that 1:1 absence to presence is far too low when including the 'local'
# absences. 
m30 <- bind_rows(m30, abs)
m30 <- filter(m30, st_is(m30, "POINT")) |>
  select(Occurrence)

distOrder_PAratio_simulator(
  x = m30, distOrder = 0, PAratio = 3.0,
  resolution = 10, iteration = 1, se_prediction = FALSE,
  train_split = 0.9, p2proc = '../data/spatial/processed'
  )


process_plss(
  path = '.', 
  pathOut = '../geodata', 
  tile_cells = tile_cells 
)

getwd()

```



# playing with count prediction 


```{r Partition data and clean for modelling}

ct <- st_read('../data/Data4modelling/30m-count-iter1.gpkg', quiet = TRUE) |>
  select(Prsnc_M, Prsnc_J) |>
  mutate(across(starts_with("Prsnc_"), ~ round(.x / 100, 0)))

# extract independent variables to data set . 
resolution = "1arc"
train_split = 0.8; p2proc = '../data/spatial/processed'
rast_dat <- rastReader(paste0('dem_', resolution), p2proc) 
r <- rast('../results/suitability_maps/1arc-Iteration1-PA1:3DO:2-Pr.tif')

df <- dplyr::bind_cols(
  ct, 
  dplyr::select(terra::extract(rast_dat, ct), -ID), 
  Pr.SuitHab = terra::extract(r, ct)$X1
) |> 
  mutate(Prsnc_All = Prsnc_J + Prsnc_M, .before = 1) |>
  tidyr::drop_na() |>
  select(-Prsnc_J, -Prsnc_M)


data_split <- df |>
  rsample::initial_split(strata = Prsnc_All, prop = 4/5) 

train <- rsample::training(data_split)
test  <- rsample::testing(data_split)

# NOT CURRENTLY WORKING 
# indices_knndm <- CAST::knndm(
#   sf::st_as_sf(train, coords = c('Longitude', 'Latitude'), crs = 32613) |> select(-Prsnc_M),
#  rast_dat,
#  space = 'geographic',
#  k=10)


```

Modelling count data has not been very successful, nor did the field work really develop any strong insights on what drives the abundance of the plants. 
Rather the abundance seemed random within populations, and between - with the exception of Cocheotopa where plants were much smaller and incredibly more abundant than elsewhere. 

The null hypothesis for estimating population sizes will be the use of the arithmatic mean per populations. 

```{r arithmatic mean of counts per population}

```

The use of kriging interpolation, where we simple smooth averages out between the plots, will be our null hypothesis for any spatial estimates of population size. 

```{r idw data}
library(gstat)

krig_preds <- data.frame(
    'Observed' = test$Prsnc_All, 
    'Predicted' = gstat::krige(Prsnc_All ~ 1, train, newdata = test)$var1.pred,
    'Pr.suit' = test$Pr.SuitHab
)

ggplot(data = krig_preds) + 
  geom_point(aes(x = Observed, y = Predicted, color = Pr.suit)) +
  theme(aspect.ratio = 1) +
  coord_fixed() + 
  geom_abline()

Metrics::mae(krig_preds$Observed, krig_preds$Predicted)
```



```{r Boosting models}
train <- sf::st_drop_geometry(train) 
test  <- rsample::testing(data_split)

rec <- recipes::recipe(Prsnc_All ~ ., data = train) 
rs <- rsample::bootstraps(train, times = 10)

tune_gr <- parsnip::boost_tree( 
  mode = 'regression',
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(), 
  loss_reduction = tune(),
  learn_rate = tune(), 
  stop_iter = tune()
)
```

```{r xgboost poisson, eval = F}

xgb_poisson <- tune_gr  |>
  parsnip::set_engine("xgboost", objective = "count:poisson") 

xgb_poisson_gr <- xgb_poisson |>
  # going to use a typical grid which can support subgrid searchers. 
  extract_parameter_set_dials() |>
  dials::grid_regular(levels = 4)

future::plan(multisession, workers = parallel::detectCores())
params_xg_poisson <- xgb_poisson |>
      finetune::tune_race_anova(
        rec,
        resamples = rs,
        metrics = yardstick::metric_set(yardstick::mae), 
        grid = xb_poisson_gr
      )

best_param <- tune::select_best(params_xg_poisson, metric = "mae")

xg_poisson <- params_xg_poisson |>
  tune::finalize_model(best_param)

xgb_pois_fit <- xg_poisson %>% 
  fit(Prsnc_All ~ ., data = train)

test_pred <- data.frame(
    'Observed' = test$Prsnc_All, 
    'Predicted' = stats::predict(xgb_fit, new_data = test),
    'Pr.suit' = test$Pr.SuitHab
)

ggplot(data = test_pred) + 
  geom_point(aes(x = Observed, y = .pred, color = Pr.suit)) +
  theme(aspect.ratio = 1) 

vip::vip(xgb_fit, num_features = 20)
Metrics::mae(test_pred$Observed, test_pred$.pred)
Metrics::rmse(test_pred$Observed, test_pred$.pred)
```

```{r xgboost with tweedie distribution}

xgb_reg_model <- tune_gr |>
    parsnip::set_engine("xgboost", objective = "reg:tweedie")

bt_grid <- xgb_reg_model %>%
  # going to use a typical grid which can support subgrid searchers. 
  extract_parameter_set_dials() %>% 
  dials::grid_regular(levels = 4)

future::plan(multisession, workers = parallel::detectCores())
xbg_tweedie_params <- xgb_reg_model |>
      finetune::tune_race_anova(
        rec,
        resamples = rs,
        metrics = yardstick::metric_set(yardstick::mae), 
        grid = bt_grid
      )

best_param_tweedie <- tune::select_best(xbg_tweedie_params, metric = "mae")

xgb_tweedie_mod <- xbg_tweedie_params |>
  tune::finalize_model(best_param_tweedie)

xgb_fit <- xgb_tweedie_mod %>% 
  fit(Prsnc_All ~ ., data = train)

test_pred <- data.frame(
    'Observed' = test$Prsnc_All, 
    'Predicted' = stats::predict(xgb_fit, new_data = test),
    'Pr.suit' = test$Pr.SuitHab
)

ggplot(data = test_pred) + 
  geom_point(aes(x = Observed, y = .pred, color = Pr.suit)) +
  theme(aspect.ratio = 1) 

vip::vip(xgb_fit, num_features = 20)

Metrics::mae(test_pred$Observed, test_pred$.pred)
Metrics::rmse(test_pred$Observed, test_pred$.pred)
```


```{r lightgbm}

tune_gr <- parsnip::boost_tree( 
  mode = 'regression',
  mtry = integer(),
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(), 
  loss_reduction = tune(),
  learn_rate = tune()
)

lgbm_poisson_gr <- tune_gr  |>
  parsnip::set_engine("lightgbm", objective = "poisson")

gr <- lgbm_poisson_gr |>
  # going to use a typical grid which can support subgrid searchers. 
  extract_parameter_set_dials() |>
  dials::grid_regular(levels = 4)

future::plan(multisession, workers = parallel::detectCores())
params_xg_poisson <- lgbm_poisson_gr |>
      finetune::tune_race_anova(
        rec,
        resamples = rs,
        metrics = yardstick::metric_set(yardstick::mae), 
        grid = gr
      )

best_param <- tune::select_best(model_params, metric = "mae")

```

Try with mboost
```{r}

mboost_reg_model <- parsnip::boost_tree( 
  mode = 'regression',
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(), 
  loss_reduction = tune(),
  learn_rate = tune(), 
  stop_iter = tune()
) |>
    parsnip::set_engine("mboost", objective = "count:poisson")

model_params <- mboost_reg_model |>
   finetune::tune_sim_anneal(rec, 
                             metrics = yardstick::metric_set(yardstick::mae),
                   resamples = rs, iter = 15, initial = parallel::detectCores()
                   )
```


```{r try with gbm3}

trainControl <- trainControl(method="repeatedcv", number=10, repeats = 5)

  gbm_model <- caret::train(
    x = train[,-grep('Prsnc_All', colnames(train))],
    y = train$Prsnc_All,
    method = "gbm",
    distribution="poisson",
    metric = 'MAE', 
    trControl = caret::trainControl(
      method="repeatedcv", number=10, repeats = 5,
     # index = indices_knndm$indx_train,
      allowParallel = TRUE)
    )
  
  
  gbm_pred <- data.frame(
    'Observed' = test$Prsnc_M, 
    'Predicted' = predict(gbm_model, newdata=test, type="raw"), 
    'Pr.suit' = test$Pr.SuitHab
)
  
Metrics::mae(gbm_pred$Observed, gbm_pred$Predicted)
Metrics::rmse(gbm_pred$Observed, gbm_pred$Predicted)

ggplot(data = gbm_pred) + 
  geom_point(aes(x = Observed, y = Predicted, color = Pr.suit)) +
  theme(aspect.ratio = 1) +
  coord_fixed() + 
  geom_abline()

```



